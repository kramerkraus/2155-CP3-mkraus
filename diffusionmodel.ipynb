{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0EsKXGqq6PuLAJBRTSngY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kramerkraus/2155-CP3-mkraus/blob/main/diffusionmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bMjXOpysu0la"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1. Small MLP backbone (denoiser)\n",
        "# --------------------------------------------------\n",
        "\n",
        "class MLPDenoiser(nn.Module):\n",
        "    def __init__(self, input_dim=37, hidden=256):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim + 1 + input_dim, hidden), # x + t + mask\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, input_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_t, t, mask):\n",
        "        \"\"\"\n",
        "        x_t: noised data\n",
        "        t: time step (batch, 1)\n",
        "        mask: binary mask (1 = observed, 0 = missing)\n",
        "        \"\"\"\n",
        "        t = t / 1000.0       # normalize time\n",
        "        inp = torch.cat([x_t, t, mask], dim=1)\n",
        "        return self.net(inp)\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2. Diffusion core (betas, schedules, noise)\n",
        "# --------------------------------------------------\n",
        "\n",
        "def make_beta_schedule(T=1000, start=1e-4, end=0.02):\n",
        "    return torch.linspace(start, end, T)\n",
        "\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self, input_dim, timesteps=1000):\n",
        "        super().__init__()\n",
        "        self.T = timesteps\n",
        "        betas = make_beta_schedule(timesteps)\n",
        "        alphas = 1.0 - betas\n",
        "        alphas_cum = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "        self.register_buffer(\"betas\", betas)\n",
        "        self.register_buffer(\"alphas\", alphas)\n",
        "        self.register_buffer(\"alphas_cum\", alphas_cum)\n",
        "\n",
        "        self.model = MLPDenoiser(input_dim=input_dim)\n",
        "\n",
        "    # -------------------------\n",
        "    # q(x_t | x_0)\n",
        "    # -------------------------\n",
        "    def q_sample(self, x0, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0)\n",
        "        a_bar = self.alphas_cum[t].unsqueeze(1)\n",
        "        return torch.sqrt(a_bar) * x0 + torch.sqrt(1 - a_bar) * noise, noise\n",
        "\n",
        "    # -------------------------\n",
        "    # Training step\n",
        "    # -------------------------\n",
        "    def forward(self, x0, mask):\n",
        "        \"\"\"\n",
        "        x0 : clean data (batch, D)\n",
        "        mask : 1 = observed, 0 = missing\n",
        "        \"\"\"\n",
        "        B = x0.shape[0]\n",
        "        device = x0.device\n",
        "\n",
        "        # Random time t for each sample\n",
        "        t = torch.randint(0, self.T, (B,), device=device)\n",
        "\n",
        "        # Noise forward\n",
        "        xt, noise = self.q_sample(x0, t, noise=None)\n",
        "\n",
        "        # Condition on observed values\n",
        "        xt = xt * (1 - mask) + x0 * mask\n",
        "\n",
        "        # Predict noise\n",
        "        noise_pred = self.model(xt, t.unsqueeze(1).float(), mask)\n",
        "\n",
        "        # Loss only on missing entries\n",
        "        loss = ((noise_pred - noise) ** 2 * (1 - mask)).mean()\n",
        "        return loss\n",
        "\n",
        "    # -------------------------\n",
        "    # Sampling / imputation\n",
        "    # -------------------------\n",
        "    @torch.no_grad()\n",
        "    def sample(self, x_obs, mask):\n",
        "        \"\"\"x_obs has missing entries set to anything (will overwrite them).\"\"\"\n",
        "        x = torch.randn_like(x_obs)\n",
        "\n",
        "        for t in reversed(range(self.T)):\n",
        "            bt = self.betas[t]\n",
        "            at = self.alphas[t]\n",
        "            a_bar = self.alphas_cum[t]\n",
        "\n",
        "            # Conditioner: always respect observed values\n",
        "            x = x * (1 - mask) + x_obs * mask\n",
        "\n",
        "            noise_pred = self.model(x, torch.tensor([[t]], device=x.device), mask)\n",
        "\n",
        "            # DDPM update step\n",
        "            coef1 = 1 / torch.sqrt(at)\n",
        "            coef2 = (1 - at) / torch.sqrt(1 - a_bar)\n",
        "\n",
        "            x = coef1 * (x - coef2 * noise_pred)\n",
        "\n",
        "            if t > 0:\n",
        "                x += torch.sqrt(bt) * torch.randn_like(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Diffusion(input_dim=37, timesteps=500).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "u8bw1CxnvuMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(200):\n",
        "    for batch, mask in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        loss = model(batch, mask)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    print(f\"epoch {epoch} | loss {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "T9GDCTjhvzcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_incomplete = X_test_imputed[i]      # has -1 replaced by something\n",
        "mask = (X_test_missing_mask[i] == 0)  # convert your mask to 1/0 observed/missing\n",
        "\n",
        "x_incomplete = torch.tensor(x_incomplete).float().to(device)\n",
        "mask = torch.tensor(mask).float().to(device)\n",
        "\n",
        "samples = []\n",
        "\n",
        "for _ in range(20):\n",
        "    x_gen = model.sample(x_incomplete.unsqueeze(0), mask.unsqueeze(0))\n",
        "    samples.append(x_gen.cpu().numpy())\n",
        "\n",
        "samples = np.array(samples)   # (20, 1, 37)"
      ],
      "metadata": {
        "id": "qXLJC4lUv3E8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}